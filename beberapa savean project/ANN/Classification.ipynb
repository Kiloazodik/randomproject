{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Acer\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 8)\n",
      "[[-1.45528465e+07 -3.05014950e+00 -1.24457169e+01  1.69498961e+00]\n",
      " [-1.45528461e+07 -3.23163925e+00  2.05665151e+01  6.30518659e-01]\n",
      " [-1.45528461e+07 -3.20840843e+00  7.54834329e+00  2.09272675e+00]\n",
      " ...\n",
      " [-1.45528449e+07 -3.30578440e+00 -6.43626411e+00 -2.88568481e-01]\n",
      " [-1.45528457e+07 -3.21475605e+00 -2.43687977e+00 -9.87204246e-01]\n",
      " [-1.45528449e+07 -3.52652057e+00 -4.45736411e+00  3.54442272e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def loadData():\n",
    "\n",
    "    data = pd.read_csv(\"E202-COMP7117-TD01-00 - classification.csv\")\n",
    "\n",
    "    if data.isna().values.any() == True:\n",
    "        data = data.dropna()\n",
    "    \n",
    "    dataInput = data[[\"volatile acidity\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \n",
    "                      \"density\", \"pH\",\"sulphates\",\"alcohol\"]]\n",
    "    target = data[[\"quality\"]]\n",
    "    \n",
    "    print(dataInput.shape)\n",
    "    \n",
    "    for y in dataInput:\n",
    "        if(y == \"free sulfur dioxide\" ):\n",
    "#             print(y)\n",
    "#             print(dataInput[y][0])\n",
    "            for j in range(len(dataInput)):\n",
    "                if(dataInput[y][j] == \"High\"):\n",
    "                    dataInput.at[j, y] = 3\n",
    "                elif(dataInput[y][j] == \"Medium\"):\n",
    "                    dataInput.at[j, y] = 2\n",
    "                elif(dataInput[y][j] == \"Low\"):\n",
    "                    dataInput.at[j, y] = 1\n",
    "                else:\n",
    "                    dataInput.at[j, y] = 0\n",
    "        elif(y == \"density\"):\n",
    "#             print(y)\n",
    "#             print(dataInput[y][0])\n",
    "            for j in range(len(dataInput)):\n",
    "                if(dataInput[y][j] == \"Very High\"):\n",
    "                    dataInput.at[j, y] = 0\n",
    "                elif(dataInput[y][j] == \"High\"):\n",
    "                    dataInput.at[j, y] = 3\n",
    "                elif(dataInput[y][j] == \"Medium\"):\n",
    "                    dataInput.at[j, y] = 2\n",
    "                elif(dataInput[y][j] == \"Low\"):\n",
    "                    dataInput.at[j, y] = 1\n",
    "        elif(y == \"pH\"):\n",
    "#             print(y)\n",
    "#             print(dataInput[[y]].values[0])\n",
    "            for j in range(len(dataInput)):\n",
    "                    if(dataInput[y][j] == \"Very Basic\"):\n",
    "                        dataInput.at[j, y] = 3\n",
    "                    elif(dataInput[y][j] == \"Normal\"):\n",
    "                        dataInput.at[j, y] = 2\n",
    "                    elif(dataInput[y][j] == \"Very Acidic\"):\n",
    "                        dataInput.at[j, y] = 1\n",
    "                    else:\n",
    "                        dataInput.at[j, y] = 0\n",
    "    \n",
    "    #normalisasi\n",
    "    feature = MinMaxScaler().fit_transform(dataInput)\n",
    "    target = OneHotEncoder(sparse=False).fit_transform(target)\n",
    "    \n",
    "    #PCA\n",
    "    result = PCA(n_components=4).fit_transform(dataInput)\n",
    "        \n",
    "    return result, target\n",
    "\n",
    "\n",
    "inputData, target = loadData()\n",
    "print(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layers = {\n",
    "    \"input\": 4, #8 different kind of data\n",
    "    \"hidden\": 500,\n",
    "    \"output\": 5 # decent, fair, fine, good, great\n",
    "}\n",
    "\n",
    "weights = {\n",
    "    'input_to_hidden' : tf.Variable(tf.random_normal([layers['input'], layers['hidden']])),\n",
    "    'hidden_to_output' : tf.Variable(tf.random_normal([layers['hidden'], layers['output']]))\n",
    "}\n",
    "\n",
    "bias = {\n",
    "    'input_to_hidden' : tf.Variable(tf.random_normal([layers['hidden']])),\n",
    "    'hidden_to_output' : tf.Variable(tf.random_normal([layers['output']]))\n",
    "}\n",
    "\n",
    "inputPlaceholder = tf.placeholder(tf.float32, [None, layers[\"input\"]])\n",
    "outputPlacehlder = tf.placeholder(tf.float32, [None, layers[\"output\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 4)\n",
      "(1599, 5)\n",
      "(1151, 4)\n",
      "(288, 4)\n",
      "(160, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def feedForward(inputData):\n",
    "    #first\n",
    "    input_to_hidden_bias = tf.matmul(inputData, weights['input_to_hidden']) + bias['input_to_hidden']\n",
    "    activated_input_to_hidden = tf.nn.sigmoid(input_to_hidden_bias)\n",
    "    #second\n",
    "    hidden_to_output_bias = tf.matmul(activated_input_to_hidden, weights['hidden_to_output']) + bias['hidden_to_output']\n",
    "    activated_hidden_to_output = tf.nn.sigmoid(hidden_to_output_bias)\n",
    "\n",
    "    return activated_hidden_to_output\n",
    "\n",
    "predict = feedForward(inputPlaceholder)\n",
    "\n",
    "epoch = 5000\n",
    "\n",
    "error = tf.reduce_mean(0.5 * ( outputPlacehlder - predict ) ** 2)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(error)\n",
    "\n",
    "inputTrain, inputTest, outputTrain, outputTest = train_test_split(inputData, target, test_size=0.1)\n",
    "print(inputData.shape)\n",
    "print(target.shape)\n",
    "inputTrain, inputValidationTest, outputTrain, outputValidationTest = train_test_split(inputTrain, outputTrain, test_size=0.2)\n",
    "\n",
    "print(inputTrain.shape)\n",
    "print(inputValidationTest.shape)\n",
    "print(inputTest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 100, loss : 0.267198771238327\n",
      "Epoch : 200, loss : 0.26713284850120544\n",
      "Epoch : 300, loss : 0.2671055495738983\n",
      "Epoch : 400, loss : 0.2670874297618866\n",
      "Epoch : 500, loss : 0.26707392930984497\n",
      "Epoch : 600, loss : 0.267062783241272\n",
      "Epoch : 700, loss : 0.26705461740493774\n",
      "Epoch : 800, loss : 0.2670477330684662\n",
      "Epoch : 900, loss : 0.2670430839061737\n",
      "Epoch : 1000, loss : 0.26703837513923645\n",
      "Epoch : 1100, loss : 0.2670351564884186\n",
      "Epoch : 1200, loss : 0.26703253388404846\n",
      "Epoch : 1300, loss : 0.2670293152332306\n",
      "Epoch : 1400, loss : 0.2670278251171112\n",
      "Epoch : 1500, loss : 0.26702603697776794\n",
      "Epoch : 1600, loss : 0.2670249044895172\n",
      "Epoch : 1700, loss : 0.26702308654785156\n",
      "Epoch : 1800, loss : 0.2670222520828247\n",
      "Epoch : 1900, loss : 0.2670210003852844\n",
      "Epoch : 2000, loss : 0.26702049374580383\n",
      "Epoch : 2100, loss : 0.26701998710632324\n",
      "Epoch : 2200, loss : 0.26701951026916504\n",
      "Epoch : 2300, loss : 0.2670186161994934\n",
      "Epoch : 2400, loss : 0.26701828837394714\n",
      "Epoch : 2500, loss : 0.26701784133911133\n",
      "Epoch : 2600, loss : 0.2670176029205322\n",
      "Epoch : 2700, loss : 0.26701638102531433\n",
      "Epoch : 2800, loss : 0.26701611280441284\n",
      "Epoch : 2900, loss : 0.2670159339904785\n",
      "Epoch : 3000, loss : 0.26701560616493225\n",
      "Epoch : 3100, loss : 0.2670154571533203\n",
      "Epoch : 3200, loss : 0.267015278339386\n",
      "Epoch : 3300, loss : 0.26701462268829346\n",
      "Epoch : 3400, loss : 0.2670145034790039\n",
      "Epoch : 3500, loss : 0.26701438426971436\n",
      "Epoch : 3600, loss : 0.2670142352581024\n",
      "Epoch : 3700, loss : 0.26701411604881287\n",
      "Epoch : 3800, loss : 0.26701390743255615\n",
      "Epoch : 3900, loss : 0.267013818025589\n",
      "Epoch : 4000, loss : 0.2670137286186218\n",
      "Epoch : 4100, loss : 0.2670135796070099\n",
      "Epoch : 4200, loss : 0.2670135200023651\n",
      "Epoch : 4300, loss : 0.26701340079307556\n",
      "Epoch : 4400, loss : 0.2670133411884308\n",
      "Epoch : 4500, loss : 0.267013281583786\n",
      "Epoch : 4600, loss : 0.26701319217681885\n",
      "Epoch : 4700, loss : 0.2670131325721741\n",
      "Epoch : 4800, loss : 0.2670130729675293\n",
      "Epoch : 4900, loss : 0.26701298356056213\n",
      "Epoch : 5000, loss : 0.26701292395591736\n",
      "Tensor(\"Equal_9:0\", shape=(?,), dtype=bool)\n",
      "accuracy: 3.750000149011612\n",
      "0.266993\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #70% of the dataset - train\n",
    "    for i in range(1, epoch + 1) :\n",
    "        train_dict = {\n",
    "            inputPlaceholder : inputTrain,\n",
    "            outputPlacehlder : outputTrain\n",
    "        }\n",
    "        sess.run(train, feed_dict = train_dict)\n",
    "\n",
    "        loss = sess.run(error, feed_dict = train_dict)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch : {}, loss : {}\".format(i, loss))\n",
    "        \n",
    "        #20% of the dataset - valid\n",
    "        if i % 500 == 0:\n",
    "            \n",
    "            validation_dict = {\n",
    "                inputPlaceholder : inputValidationTest,\n",
    "                outputPlacehlder : outputValidationTest\n",
    "            }\n",
    "            sess.run(train, feed_dict = validation_dict)\n",
    "\n",
    "            Validationloss = sess.run(error, feed_dict = validation_dict)\n",
    "            \n",
    "#             print(\"Validation Epoch : {}, loss : {}\".format(i, Validationloss))\n",
    "            if i == 500:\n",
    "                lowestValidationLoss = Validationloss\n",
    "            \n",
    "                f=open(\"lowestValidationLoss.txt\", \"w\")\n",
    "                f.write(str(lowestValidationLoss))\n",
    "                f.close()\n",
    "            \n",
    "            if Validationloss < lowestValidationLoss:\n",
    "                lowestValidationLoss = Validationloss\n",
    "                f=open(\"lowestValidationLoss.txt\", \"w\")\n",
    "                f.write(str(lowestValidationLoss))\n",
    "                f.close()\n",
    "            \n",
    "                \n",
    "    #10% of the dataset - evaluation\n",
    "    matches = tf.equal(tf.argmax(outputPlacehlder,axis = 1), tf.argmax(predict,axis = 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "    print(matches)\n",
    "    feed_test = {\n",
    "        inputPlaceholder: inputTest,\n",
    "        outputPlacehlder: outputTest\n",
    "    }\n",
    "\n",
    "    print(\"accuracy: {}\".format(sess.run(accuracy, feed_dict = feed_test) *100 ))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
